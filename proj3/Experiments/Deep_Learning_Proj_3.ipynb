{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_IN_COLAB = False\n",
    "DATASET_PATH = \"../Dataset/bedroom/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import DDPMPipeline, DDPMScheduler, UNet2DModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from Enum import enum\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSUNBedroomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Traverse the directory to get image paths\n",
    "        for subdir, _, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith((\"png\", \"jpg\", \"jpeg\")):\n",
    "                    self.image_paths.append(os.path.join(subdir, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "class ModelType(enum):\n",
    "    UNet = 1\n",
    "\n",
    "\n",
    "class NoiseSchedulerType(enum):\n",
    "    DDPMSched = 1\n",
    "\n",
    "\n",
    "class OptimizerType(enum):\n",
    "    Adam = 1\n",
    "\n",
    "\n",
    "def namestr(obj, namespace) -> str:\n",
    "    \"\"\"Get name of a variable as a string\"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "\n",
    "def getOptimizer(optimzierType: OptimizerType, model, params):\n",
    "    match optimzierType:\n",
    "        case OptimizerType.Adam:\n",
    "            return torch.optim.Adam(model.parameters(), **params)\n",
    "\n",
    "\n",
    "def getScheduler(schedulerType: NoiseSchedulerType, params):\n",
    "    match schedulerType:\n",
    "        case NoiseSchedulerType.DDPMSched:\n",
    "            return DDPMScheduler(**params)\n",
    "\n",
    "\n",
    "def getModel(modelType: ModelType, device):\n",
    "    match modelType:\n",
    "        case ModelType.UNet:\n",
    "            model = UNet2DModel(\n",
    "                sample_size=32,  # the target image resolution\n",
    "                in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "                out_channels=3,  # the number of output channels\n",
    "                layers_per_block=1,\n",
    "                block_out_channels=(32, 64, 128),\n",
    "                down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "                up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "            )\n",
    "            model.to(device)\n",
    "            return model\n",
    "\n",
    "\n",
    "def trainModel(model, num_epochs, dataloader, optimizer, device, noise_scheduler):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.num_train_timesteps, (batch.size(0),), device=device\n",
    "            ).long()\n",
    "            noise = torch.randn_like(batch)\n",
    "            noisy_images = noise_scheduler.add_noise(batch, noise, timesteps)\n",
    "\n",
    "            noise_pred = model(noisy_images, timesteps).sample\n",
    "\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def generateImages(model, device, noise_scheduler, num_epochs, output_dir, num_images):\n",
    "    model.eval()\n",
    "\n",
    "    generated_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(num_images), desc=\"Generating Images\"):\n",
    "            noisy_image = torch.randn(1, 3, 32, 32, device=device)\n",
    "            for t in reversed(\n",
    "                range(noise_scheduler.config.num_train_timesteps)\n",
    "            ):  # Access via config\n",
    "                timesteps = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "                model_output = model(noisy_image, timesteps)\n",
    "\n",
    "                step_result = noise_scheduler.step(model_output.sample, t, noisy_image)\n",
    "                noisy_image = step_result.prev_sample\n",
    "\n",
    "            generated_image = noisy_image.squeeze(0).cpu()\n",
    "            generated_images.append(generated_image)\n",
    "\n",
    "    # Saving generated images\n",
    "    output_dir = f\"generated_images_{num_epochs}_epochs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for idx, image in enumerate(generated_images):\n",
    "        save_image(image, f\"{output_dir}/generated_image_{namestr(model)}_{idx+1}.png\")\n",
    "\n",
    "    print(f\"{num_images} images generated and saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
    "\n",
    "# Create dataset\n",
    "dataset = LSUNBedroomDataset(root_dir=\"DATASET_PATH\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and Generate images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
